{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d749596",
   "metadata": {},
   "source": [
    "# Step 7: Feature Engineering Based on SHAP\n",
    "Engineer new high-impact features and drop weak ones identified from SHAP analysis.\n",
    "\n",
    "## Objectives\n",
    "- Use insights from our SHAP analysis (step6) to eliminate low-impact features\n",
    "- Create new engineered features focused on high-impact feature interactions\n",
    "- Generate an improved feature set (v2) for model retraining\n",
    "- Focus on creating features that leverage nonlinear relationships identified by SHAP\n",
    "\n",
    "## Background\n",
    "SHAP analysis from the previous notebook identified several key opportunities:\n",
    "- `Duration` and `Heart_Rate` have the strongest impact on predictions\n",
    "- Some features like `BMI` and `Height` contribute minimal predictive power\n",
    "- Interaction effects between variables (e.g., temperature and heart rate) have high potential\n",
    "- Age and intensity show moderately strong impacts worth exploring further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55717e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the feature-engineered training and test datasets from step2b\n",
    "train = pd.read_csv(\"datasets/train_fe.csv\")\n",
    "test = pd.read_csv(\"datasets/test_fe.csv\")\n",
    "\n",
    "# Create fallback IDs in case 'id' column is not present\n",
    "# This is a safeguard to ensure we can track samples through our pipeline\n",
    "train_id = pd.Series(range(len(train)), name=\"id\")\n",
    "test_id = pd.Series(range(len(test)), name=\"id\")\n",
    "\n",
    "# Split features and target variable for processing\n",
    "y_train = train['Calories']  # Target variable (what we're predicting)\n",
    "X_train = train.drop(columns=['Calories'])  # Feature matrix for training\n",
    "X_test = test.copy()  # Test features (no target available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features identified as low-impact by SHAP analysis (step6)\n",
    "# These features showed minimal contribution to the model predictions:\n",
    "# - 'BMI': Despite being intuitively useful, SHAP values were nearly zero\n",
    "# - 'Height': Low impact unless paired with other features\n",
    "features_to_drop = ['BMI', 'Height']  \n",
    "\n",
    "# Remove these features from both training and test datasets\n",
    "X_train.drop(columns=features_to_drop, inplace=True)\n",
    "X_test.drop(columns=features_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(df):\n",
    "    \"\"\"\n",
    "    Create new engineered features based on SHAP insights to capture important relationships.\n",
    "    \n",
    "    Key feature engineering strategies applied:\n",
    "    1. Ratio features - capture relationships between important variables\n",
    "    2. Interaction features - multiply high-impact features together\n",
    "    3. Composite features - combine multiple variables in meaningful ways\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the original features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    # Duration per Age: Captures exercise intensity relative to age\n",
    "    # Older people burning same calories in same duration = higher intensity\n",
    "    df['Duration_per_Age'] = df['Duration'] / df['Age']\n",
    "    \n",
    "    # Heart Rate × Duration: Captures total cardiac output over workout period\n",
    "    # Combines two highest-impact features identified by SHAP\n",
    "    df['HRxDuration'] = df['Heart_Rate'] * df['Duration']\n",
    "    \n",
    "    # Temperature-Heart interaction normalized by Intensity\n",
    "    # Captures efficiency of the body's heat response relative to effort\n",
    "    df['TempHeart_per_Intensity'] = df['TempHeart'] / (df['Intensity'] + 1e-5)  # Add small constant to avoid division by zero\n",
    "    \n",
    "    # Heart Rate relative to Body Temperature\n",
    "    # Captures cardiovascular efficiency relative to thermal response\n",
    "    df['HR_per_BodyTemp'] = df['Heart_Rate'] / (df['Body_Temp'] + 1e-5)\n",
    "    \n",
    "    # Complex interaction: (Temperature × Heart Rate × Age) / Weight\n",
    "    # Captures age-adjusted thermal-cardiac response relative to body mass\n",
    "    df['TempHeart_Age_per_Weight'] = (df['TempHeart'] * df['Age']) / (df['Weight'] + 1e-5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both training and test sets\n",
    "X_train = create_new_features(X_train)\n",
    "X_test = create_new_features(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_fe_v2.csv and test_fe_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Restore the target variable to the training dataset before saving\n",
    "X_train['Calories'] = y_train\n",
    "\n",
    "# Insert ID columns at the beginning of both dataframes\n",
    "X_train.insert(0, 'id', train_id)\n",
    "X_test.insert(0, 'id', test_id)\n",
    "\n",
    "# Save the enhanced feature sets as new files with '_v2' suffix\n",
    "# These will be used in subsequent modeling steps\n",
    "X_train.to_csv(\"datasets/train_fe_v2.csv\", index=False)\n",
    "X_test.to_csv(\"datasets/test_fe_v2.csv\", index=False)\n",
    "print(\"Saved: train_fe_v2.csv and test_fe_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67be998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id                          0\n",
       " Age                         0\n",
       " Weight                      0\n",
       " Duration                    0\n",
       " Heart_Rate                  0\n",
       " Body_Temp                   0\n",
       " Sex_male                    0\n",
       " Intensity                   0\n",
       " TempHeart                   0\n",
       " Duration2                   0\n",
       " Duration_per_Age            0\n",
       " HRxDuration                 0\n",
       " TempHeart_per_Intensity     0\n",
       " HR_per_BodyTemp             0\n",
       " TempHeart_Age_per_Weight    0\n",
       " Calories                    0\n",
       " dtype: int64,\n",
       " id                          0\n",
       " Age                         0\n",
       " Weight                      0\n",
       " Duration                    0\n",
       " Heart_Rate                  0\n",
       " Body_Temp                   0\n",
       " Sex_male                    0\n",
       " Intensity                   0\n",
       " TempHeart                   0\n",
       " Duration2                   0\n",
       " Duration_per_Age            0\n",
       " HRxDuration                 0\n",
       " TempHeart_per_Intensity     0\n",
       " HR_per_BodyTemp             0\n",
       " TempHeart_Age_per_Weight    0\n",
       " dtype: int64,\n",
       "                   id            Age         Weight       Duration  \\\n",
       " count  750000.000000  750000.000000  750000.000000  750000.000000   \n",
       " mean   374999.500000      41.420404      75.145668      15.421015   \n",
       " std    216506.495284      15.175049      13.982704       8.354095   \n",
       " min         0.000000      20.000000      36.000000       1.000000   \n",
       " 25%    187499.750000      28.000000      63.000000       8.000000   \n",
       " 50%    374999.500000      40.000000      74.000000      15.000000   \n",
       " 75%    562499.250000      52.000000      87.000000      23.000000   \n",
       " max    749999.000000      79.000000     132.000000      30.000000   \n",
       " \n",
       "           Heart_Rate      Body_Temp      Intensity      TempHeart  \\\n",
       " count  750000.000000  750000.000000  750000.000000  750000.000000   \n",
       " mean       95.483995      40.036253      10.547400    3828.687447   \n",
       " std         9.449845       0.779875      12.237710     437.967454   \n",
       " min        67.000000      37.100000       2.714286    2485.700000   \n",
       " 25%        88.000000      39.600000       4.521739    3497.400000   \n",
       " 50%        95.000000      40.300000       6.214286    3838.000000   \n",
       " 75%       103.000000      40.700000      10.750000    4171.500000   \n",
       " max       128.000000      41.500000     108.000000    5286.400000   \n",
       " \n",
       "            Duration2  Duration_per_Age    HRxDuration  \\\n",
       " count  750000.000000     750000.000000  750000.000000   \n",
       " mean      307.598511          0.427308    1541.562606   \n",
       " std       266.928280          0.295606     932.453480   \n",
       " min         1.000000          0.012658      67.000000   \n",
       " 25%        64.000000          0.194444     728.000000   \n",
       " 50%       225.000000          0.375000    1455.000000   \n",
       " 75%       529.000000          0.589744    2323.000000   \n",
       " max       900.000000          1.500000    3840.000000   \n",
       " \n",
       "        TempHeart_per_Intensity  HR_per_BodyTemp  TempHeart_Age_per_Weight  \\\n",
       " count            750000.000000    750000.000000             750000.000000   \n",
       " mean                623.282023         2.382151               2177.175465   \n",
       " std                 343.645498         0.201433                936.154562   \n",
       " min                  37.099994         1.687657                516.944491   \n",
       " 25%                 317.599724         2.233502               1455.299824   \n",
       " 50%                 605.999001         2.375978               1997.976390   \n",
       " 75%                 931.497960         2.530712               2722.095275   \n",
       " max                1244.996835         3.262599               7887.419598   \n",
       " \n",
       "             Calories  \n",
       " count  750000.000000  \n",
       " mean        4.141144  \n",
       " std         0.963231  \n",
       " min         0.693147  \n",
       " 25%         3.555348  \n",
       " 50%         4.356709  \n",
       " 75%         4.919981  \n",
       " max         5.752573  )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the newly created files to verify their contents\n",
    "# This also guards against any kernel resets during the process\n",
    "train_v2 = pd.read_csv(\"datasets/train_fe_v2.csv\")\n",
    "test_v2 = pd.read_csv(\"datasets/test_fe_v2.csv\")\n",
    "\n",
    "# Perform validation checks on the new datasets:\n",
    "# 1. Check for any missing values that might have been introduced\n",
    "missing_train = train_v2.isnull().sum()\n",
    "missing_test = test_v2.isnull().sum()\n",
    "\n",
    "# 2. Generate summary statistics to verify the new features look reasonable\n",
    "# This helps catch any anomalies like extreme outliers or unexpected distributions\n",
    "summary_train = train_v2.describe()\n",
    "\n",
    "# Display the validation results\n",
    "missing_train, missing_test, summary_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12538efa",
   "metadata": {},
   "source": [
    "### Top Engineered Features (stats look solid):\n",
    "\n",
    "Based on the summary statistics, our engineered features show promising distributions:\n",
    "\n",
    "| Feature                    | Mean | Notes                            |\n",
    "| -------------------------- | ---- | -------------------------------- |\n",
    "| `Duration_per_Age`         | 0.43 | Good spread (0.01 – 1.5)         |\n",
    "| `HRxDuration`              | 1541 | Multiplied range makes sense     |\n",
    "| `TempHeart_per_Intensity`  | 623  | Strong nonlinear range           |\n",
    "| `HR_per_BodyTemp`          | 2.38 | Tight spread – no outliers       |\n",
    "| `TempHeart_Age_per_Weight` | 2177 | Wide spread – likely high signal |\n",
    "\n",
    "All features show reasonable distributions with no extreme outliers or anomalies.\n",
    "The next step will be to retrain our XGBoost model on this enhanced feature set\n",
    "and evaluate whether these changes improve our prediction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edfc4d",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "- Removed low-impact features identified by SHAP analysis\n",
    "- Created 5 new engineered features focusing on high-impact interactions\n",
    "- Generated and validated enhanced feature sets (v2)\n",
    "- Preserved data structure with appropriate IDs for consistency\n",
    "\n",
    "### Next Steps:\n",
    "1. Retrain XGBoost model using these enhanced features (step7b)\n",
    "2. Evaluate if the feature engineering improved model performance\n",
    "3. Consider additional SHAP analysis on the new model for further insights\n",
    "4. Prepare final submission with best-performing model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
