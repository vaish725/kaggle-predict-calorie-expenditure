{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ac37ed",
   "metadata": {},
   "source": [
    "# Step 3: Baseline Modeling\n",
    "Train initial regression models and evaluate using RMSLE.\n",
    "\n",
    "## Overview:\n",
    "This notebook establishes baseline performance using several regression models:\n",
    "- Linear Regression (simplest approach)\n",
    "- Random Forest (tree-based ensemble)\n",
    "- XGBoost (gradient boosting)\n",
    "- LightGBM (gradient boosting)\n",
    "\n",
    "We'll use cross-validation to get reliable performance estimates and identify the most promising algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, modeling and evaluation\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "from sklearn.linear_model import LinearRegression  # Basic linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor  # Tree-based ensemble model\n",
    "from sklearn.model_selection import cross_val_score, KFold  # For cross-validation\n",
    "from sklearn.metrics import mean_squared_log_error  # For evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3984a51",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "Make sure you're using the transformed version with log(Calories) and no 'id' column.\n",
    "\n",
    "The preprocessed data includes:\n",
    "- One-hot encoded categorical variables (Sex)\n",
    "- Log-transformed target variable (Calories)\n",
    "- No identifier columns (id) that would interfere with modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training and test datasets\n",
    "train = pd.read_csv('datasets/train_preprocessed.csv')  # Load training data\n",
    "test = pd.read_csv('datasets/test_preprocessed.csv')    # Load test data\n",
    "\n",
    "# Separate features and target variable\n",
    "y = train['Calories']  # Target variable (log-transformed calories)\n",
    "X = train.drop(columns='Calories')  # Feature matrix (all columns except target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to evaluate models using k-fold cross-validation with RMSLE metric\n",
    "def rmsle_cv(model, X, y):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Logarithmic Error via cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The machine learning model to evaluate\n",
    "    - X: Feature matrix\n",
    "    - y: Target values (already log-transformed)\n",
    "    \n",
    "    Returns:\n",
    "    - Average RMSLE across all folds\n",
    "    \"\"\"\n",
    "    # Create 5-fold cross-validation with random shuffling and fixed random seed\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Calculate RMSLE scores across all folds\n",
    "    # Note: sklearn returns negative values for error metrics when used with cross_val_score\n",
    "    # so we negate it to get positive values\n",
    "    rmsle_scores = -cross_val_score(\n",
    "        model, X, y, scoring='neg_mean_squared_log_error', cv=kf\n",
    "    )\n",
    "    \n",
    "    # Return the square root of the mean (converting from MSLE to RMSLE)\n",
    "    return np.sqrt(rmsle_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSLE: 0.04522\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Linear Regression model\n",
    "# Linear Regression is the simplest model and provides a good baseline\n",
    "lr = LinearRegression()  # Initialize the model\n",
    "lr_score = rmsle_cv(lr, X, y)  # Calculate cross-validated RMSLE score\n",
    "print(f\"Linear Regression RMSLE: {lr_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5399bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSLE: 0.01813\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Random Forest model\n",
    "# Random Forest is an ensemble of decision trees that can capture non-linear relationships\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    random_state=42,   # For reproducibility\n",
    "    n_jobs=-1          # Use all available CPU cores\n",
    ")\n",
    "rf_score = rmsle_cv(rf, X, y)  # Calculate cross-validated RMSLE score\n",
    "print(f\"Random Forest RMSLE: {rf_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d3c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSLE: 0.04522\n",
      "Random Forest RMSLE:   0.01813\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression RMSLE: {lr_score:.5f}\")\n",
    "print(f\"Random Forest RMSLE:   {rf_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e1fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSLE: 0.01741\n"
     ]
    }
   ],
   "source": [
    "# Import XGBoost library for gradient boosting\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train and evaluate XGBoost model\n",
    "# XGBoost is a powerful gradient boosting library that often performs well on tabular data\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,    # Number of boosting rounds\n",
    "    learning_rate=0.1,   # Step size shrinkage to prevent overfitting\n",
    "    max_depth=6,         # Maximum depth of trees\n",
    "    random_state=42,     # For reproducibility\n",
    "    n_jobs=-1            # Use all available CPU cores\n",
    ")\n",
    "xgb_score = rmsle_cv(xgb, X, y)  # Calculate cross-validated RMSLE score\n",
    "print(f\"XGBoost RMSLE: {xgb_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecee76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.141163\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 361\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.141466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.140724\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 359\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.140493\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 356\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.141876\n",
      "LightGBM RMSLE: 0.01764\n"
     ]
    }
   ],
   "source": [
    "# Import LightGBM library for another gradient boosting implementation\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Train and evaluate LightGBM model\n",
    "# LightGBM is a gradient boosting framework that uses tree-based learning algorithms\n",
    "# It's designed to be faster and more efficient than other implementations\n",
    "lgb = LGBMRegressor(\n",
    "    n_estimators=100,    # Number of boosting rounds\n",
    "    learning_rate=0.1,   # Step size shrinkage to prevent overfitting\n",
    "    max_depth=6,         # Maximum depth of trees\n",
    "    random_state=42,     # For reproducibility\n",
    "    n_jobs=-1            # Use all available CPU cores\n",
    ")\n",
    "lgb_score = rmsle_cv(lgb, X, y)  # Calculate cross-validated RMSLE score\n",
    "print(f\"LightGBM RMSLE: {lgb_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Comparison:\n",
      "Linear Regression RMSLE: 0.04522\n",
      "Random Forest RMSLE:     0.01813\n",
      "XGBoost RMSLE:           0.01741\n",
      "LightGBM RMSLE:          0.01764\n"
     ]
    }
   ],
   "source": [
    "# Print a comparison of all model RMSLE scores to easily compare performance\n",
    "# Lower RMSLE values indicate better model performance\n",
    "print(\"Baseline Model Comparison:\")\n",
    "print(f\"Linear Regression RMSLE: {lr_score:.5f}\")\n",
    "print(f\"Random Forest RMSLE:     {rf_score:.5f}\")\n",
    "print(f\"XGBoost RMSLE:           {xgb_score:.5f}\")\n",
    "print(f\"LightGBM RMSLE:          {lgb_score:.5f}\")\n",
    "# From these results, we can identify which model performs best on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5718e0e",
   "metadata": {},
   "source": [
    "## 9. Train Best Model (XGBoost) on Full Data and Predict\n",
    "\n",
    "Based on our cross-validation results, XGBoost performs the best among our baseline models. We'll now:\n",
    "\n",
    "1. Train XGBoost on the entire training dataset\n",
    "2. Generate predictions on the test dataset\n",
    "3. Transform predictions back to the original scale (reversing the log transformation)\n",
    "4. Create a submission file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model (XGBoost) on full training data\n",
    "# Using all available data rather than just a training split for final model\n",
    "xgb.fit(X, y)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "# The predictions will be in log-transformed scale\n",
    "test_preds_log = xgb.predict(test)\n",
    "\n",
    "# Reverse the log1p transform to get predictions in the original scale\n",
    "# We use expm1() which is the inverse of log1p()\n",
    "# This converts our predictions from log(calories+1) back to calories\n",
    "test_preds = np.expm1(test_preds_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test IDs - we need these to match predictions with the correct test instances\n",
    "# We stored IDs separately when preprocessing since they weren't needed for modeling\n",
    "test_ids = pd.read_csv('datasets/test_ids.csv')['id']\n",
    "\n",
    "# Prepare submission dataframe with two columns:\n",
    "# - id: The identifier for each test instance\n",
    "# - Calories: The predicted calorie expenditure (in original scale)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Calories': test_preds\n",
    "})\n",
    "\n",
    "# Save to CSV file in the format required by Kaggle\n",
    "# The index=False parameter prevents pandas from adding an additional index column\n",
    "submission.to_csv('submission_xgb.csv', index=False)\n",
    "print(\"Submission file 'submission_xgb.csv' created.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
