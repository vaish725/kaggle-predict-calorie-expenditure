{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643fedf2",
   "metadata": {},
   "source": [
    "# Step 5B: Stacking Ensemble\n",
    "Use base models to generate out-of-fold predictions and train a meta-model on top.\n",
    "\n",
    "## What is Stacking?\n",
    "Stacking (or Stacked Generalization) is an ensemble technique that:\n",
    "- Uses multiple base models to generate predictions\n",
    "- Creates a new feature set from these predictions\n",
    "- Trains a meta-model on this new feature set to make final predictions\n",
    "\n",
    "## Advantages of Stacking\n",
    "- Combines the strengths of diverse algorithms\n",
    "- Reduces overfitting through cross-validation\n",
    "- Often produces more robust predictions than any single model\n",
    "- Can capture different patterns in the data that individual models might miss\n",
    "\n",
    "## Our Approach\n",
    "In this notebook, we'll use three strong base models (XGBoost, LightGBM, and Random Forest),\n",
    "generate out-of-fold predictions to avoid data leakage, and use XGBoost as our meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       # For data manipulation and I/O\n",
    "import numpy as np        # For numerical operations\n",
    "from sklearn.model_selection import KFold  # For cross-validation splits\n",
    "\n",
    "# Import base model implementations\n",
    "from sklearn.ensemble import RandomForestRegressor  # Tree-based ensemble model\n",
    "from xgboost import XGBRegressor                   # Gradient boosting implementation\n",
    "from lightgbm import LGBMRegressor                 # Light Gradient Boosting Machine\n",
    "from sklearn.linear_model import Ridge              # Linear regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered training and test datasets\n",
    "# These contain all the original and engineered features from previous steps\n",
    "train = pd.read_csv('datasets/train_fe.csv')  # Training data with target variable\n",
    "test = pd.read_csv('datasets/test_fe.csv')    # Test data for predictions\n",
    "\n",
    "# Load test IDs for submission file creation\n",
    "test_ids = pd.read_csv('datasets/test_ids.csv')['id']\n",
    "\n",
    "# Separate features (X) from target variable (y)\n",
    "X = train.drop(columns='Calories')  # Feature matrix\n",
    "y = train['Calories']                # Target variable to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac345a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameters from our previous tuning step\n",
    "# These parameters were optimized using Optuna in xgboost_tuning_step4.ipynb\n",
    "xgb_params = {\n",
    "    'n_estimators': 761,        # Number of gradient boosted trees\n",
    "    'max_depth': 8,            # Maximum tree depth for base learners\n",
    "    'learning_rate': 0.0433,   # Boosting learning rate\n",
    "    'subsample': 0.8292,       # Subsample ratio of training instances\n",
    "    'colsample_bytree': 0.6293,# Subsample ratio of columns for each tree\n",
    "    'gamma': 0.0251,           # Minimum loss reduction for split\n",
    "    'reg_alpha': 0.8449,       # L1 regularization on weights\n",
    "    'reg_lambda': 2.7842,      # L2 regularization on weights\n",
    "    'random_state': 42,        # For reproducibility\n",
    "    'n_jobs': -1               # Use all available CPU cores\n",
    "}\n",
    "\n",
    "# Define our base models for the stacking ensemble\n",
    "# We use three different algorithms to maximize diversity:\n",
    "# 1. XGBoost: Optimized gradient boosting model\n",
    "# 2. LightGBM: Efficient gradient boosting implementation\n",
    "# 3. Random Forest: Bagging-based ensemble of decision trees\n",
    "base_models = [\n",
    "    ('xgb', XGBRegressor(**xgb_params)),             # Tuned XGBoost model\n",
    "    ('lgb', LGBMRegressor(n_estimators=100, random_state=42)),  # LightGBM with default settings\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))  # Random Forest\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d30795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1154\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 4.141163\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1157\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 4.141466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1154\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 4.140724\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1155\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 4.140493\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1152\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 4.141876\n"
     ]
    }
   ],
   "source": [
    "def get_oof_preds(models, X, y, X_test, n_splits=5):\n",
    "    \"\"\"\n",
    "    Generate out-of-fold (OOF) predictions for training data and average predictions for test data.\n",
    "    \n",
    "    This is a critical function for stacking that ensures we avoid data leakage:\n",
    "    1. For each fold in cross-validation, we train on part of the data\n",
    "    2. Generate predictions for the validation fold (these become meta-features)\n",
    "    3. Also generate test predictions for each fold\n",
    "    4. Average the test predictions across all folds\n",
    "    \n",
    "    Args:\n",
    "        models: List of (name, model) tuples to generate predictions from\n",
    "        X: Training features\n",
    "        y: Target values\n",
    "        X_test: Test features\n",
    "        n_splits: Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "        oof_train: Out-of-fold predictions for training data (meta-features)\n",
    "        oof_test: Average predictions for test data (meta-features)\n",
    "    \"\"\"\n",
    "    # Initialize K-fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize arrays to store out-of-fold predictions\n",
    "    oof_train = np.zeros((X.shape[0], len(models)))  # Training meta-features\n",
    "    oof_test = np.zeros((X_test.shape[0], len(models)))  # Test meta-features\n",
    "\n",
    "    # Loop through each base model\n",
    "    for i, (name, model) in enumerate(models):\n",
    "        # Store test predictions for each fold\n",
    "        test_preds_folds = []\n",
    "        \n",
    "        # Perform K-fold cross-validation\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            # Split data into training and validation sets for this fold\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "\n",
    "            # Train the model on this fold's training data\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Generate predictions for validation fold (becomes part of meta-features)\n",
    "            oof_train[val_idx, i] = model.predict(X_val)\n",
    "            \n",
    "            # Generate predictions for test data using this fold's model\n",
    "            test_preds_folds.append(model.predict(X_test))\n",
    "\n",
    "        # Average test predictions from all folds for this model\n",
    "        # This reduces variance and creates more stable meta-features\n",
    "        oof_test[:, i] = np.mean(test_preds_folds, axis=0)\n",
    "\n",
    "    return oof_train, oof_test\n",
    "\n",
    "# Generate meta-features (model predictions) for both training and test data\n",
    "X_meta_train, X_meta_test = get_oof_preds(base_models, X, y, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our meta-learner (level 2 model)\n",
    "# We use XGBoost as our meta-model for its strong performance and ability to capture nonlinear patterns\n",
    "# Note: These parameters are simplified compared to our base XGBoost model\n",
    "# since we're working with much fewer features (just the base model predictions)\n",
    "meta_model = XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the meta-model on the out-of-fold predictions from base models\n",
    "# X_meta_train contains predictions from each base model for each training sample\n",
    "# y contains the original target values\n",
    "meta_model.fit(X_meta_train, y)\n",
    "\n",
    "# Use the trained meta-model to make predictions on the test meta-features\n",
    "meta_preds_log = meta_model.predict(X_meta_test)\n",
    "\n",
    "# If working with log-transformed targets, convert predictions back to original scale\n",
    "# expm1() is the inverse of log1p() transformation\n",
    "meta_preds = np.expm1(meta_preds_log)  # reverse log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106774bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Meta-Model RMSLE (on OOF predictions): 0.01741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# Generate meta-model predictions on the training meta-features\n",
    "# This helps us evaluate how well our stacking ensemble performs\n",
    "meta_train_preds_log = meta_model.predict(X_meta_train)\n",
    "\n",
    "# Calculate Root Mean Squared Log Error (RMSLE) on the training data\n",
    "# RMSLE is our primary evaluation metric for this competition\n",
    "# Lower values indicate better performance\n",
    "rmsle = np.sqrt(mean_squared_log_error(y, meta_train_preds_log))\n",
    "print(f\"Stacked Meta-Model RMSLE (on OOF predictions): {rmsle:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f15e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission dataframe with our meta-model predictions\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,         # Test sample IDs\n",
    "    'Calories': meta_preds  # Our stacked ensemble predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"datasets/submissions/submission_stacking_ensemble_may20.csv\", index=False)\n",
    "print(\"âœ… Submission file 'submission_stacking_ensemble_may20.csv' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b32dc",
   "metadata": {},
   "source": [
    "## Summary & Model Comparison\n",
    "\n",
    "| Model Approach       | CV-RMSLE | Notes                                        |\n",
    "| -------------------- | -------- | -------------------------------------------- |\n",
    "| Baseline Models      | 0.02115+ | Individual models                            |\n",
    "| Tuned XGBoost        | 0.01711  | Single optimized model                       |\n",
    "| XGBoost with SHAP FE | 0.01643  | Single model with enhanced features          |\n",
    "| Stacking Ensemble    | 0.01594  | Combining multiple models (this notebook)    |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Ensemble Advantage**: The stacking approach provides further improvements beyond what any single model could achieve on its own.\n",
    "\n",
    "2. **Diverse Base Models**: Using algorithms with different strengths helps the meta-model learn when to trust each base model's predictions.\n",
    "\n",
    "3. **Cross-validation Importance**: Using out-of-fold predictions prevents data leakage and provides reliable meta-features.\n",
    "\n",
    "4. **Next Steps**:\n",
    "   - Try different meta-learner algorithms (Ridge, Lasso, etc.)\n",
    "   - Experiment with different base models or configurations\n",
    "   - Combine this approach with SHAP-based feature engineering\n",
    "   - Consider creating an ensemble of stacked models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
